# Loss Function

## Introduction

- **Loss function** (or cost function) is a measure of how well a model is able to predict the expected outcome.
- It quantifies the difference between the predicted values and the actual values of the target variable.
- Loss functions are used to train machine learning models by optimizing their parameters to minimize the loss.

## Types of Loss Functions

1. **Regression Loss Functions**:
    - These loss functions are used for regression problems where the target variable is continuous.
    - Examples include:
        - **Squared Loss**: Measures the squared difference between the predicted and actual values.
        - **Mean Squared Error (MSE)**: Average of the squared differences between the predicted and actual values.
        - **Absolute Value Loss**: Measures the absolute difference between the predicted and actual values.
        - **0/1 Loss**: Assigns a value of 0 if the prediction is correct and 1 if it is incorrect.
        - **Huber Loss**: Combines the best properties of squared and absolute value loss.
        - **Log-Cosh Loss**: A smooth approximation of the Huber loss function.

2. **Classification Loss Functions**:
    - These loss functions are used for classification problems where the target variable is categorical.
    - Examples include:
        - **Binary Cross-Entropy Loss**: Used for binary classification problems.
        - **Categorical Cross-Entropy Loss**: Used for multi-class classification problems.
        - **Hinge Loss**: Used for support vector machines and maximum-margin classifiers.
        - **Squared Hinge Loss**: A smooth approximation of the hinge loss function.

3. **Ranking Loss Functions**:
    - These loss functions are used for ranking problems where the goal is to order items based on their relevance.
    - Examples include:
        - **Pairwise Loss**: Compares the relative ordering of pairs of items.
        - **Listwise Loss**: Considers the entire list of items and their relevance.

4. **Custom Loss Functions**:
    - Custom loss functions can be defined based on the specific requirements of the problem.
    - They can incorporate domain knowledge, business constraints, and other factors that are not captured by standard loss functions.

## Goal of Loss Minimization

- The expected **generalization loss** for a hypothesis $h$ is the expected value of the loss function over all possible inputs and outputs.
$$GenLoss_L(h) = \Sigma_{(x,y)\in \epsilon} P(x,y) L(y, h(x))$$
- The best hypothesis $h^*$ is the one that minimizes the generalization loss.
$$h^* = argmin_{h \in H} GenLoss_L(h)$$

- $P(x,y)$ is unknown, so we use the **empirical loss** on the training set to approximate the generalization loss.
$$EmpLoss_L(h) = \Sigma_{i=1}^{m} L(y_i, h(x_i))$$
- The estimated best hypothesis $\hat{h}$ is the one that minimizes the empirical loss.
$$\hat{h}^* = argmin_{h \in H} EmpLoss_L(h)$$
