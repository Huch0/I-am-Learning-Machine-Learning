# What are Trees?

## Introduction

- Trees are a class of **non-linear models** that use a **hierarchical structure** to make predictions.
- They are widely used for **regression** and **classification** tasks in machine learning.
- Trees are simple, interpretable, and can capture complex relationships between input features and the target variable.

## Tree Representation

- A tree model makes predictions by **partitioning** the input space into a set of **rectangular regions** and assigning a **constant value** to each region.
- The tree structure consists of **nodes** and **edges**:
  - **Nodes** represent decision points based on input features.
  - **Edges** connect nodes and represent the possible feature values.
- The tree structure is learned from the training data to minimize the prediction error.

## Types of Trees

- **Decision Trees**:
  - A tree model that uses a sequence of **if-else** conditions to make predictions.
  - Each internal node tests a specific feature, and each leaf node represents a target value.
  - Decision trees are simple, interpretable, and can handle both numerical and categorical features.

- **Regression Trees**:
  - A decision tree used for predicting continuous target variables.
  - The leaf nodes contain the average target value of the training examples that fall into that region.

- **Model Trees**:
  - A decision tree with linear regression models at the leaf nodes.
  - Each leaf node contains a linear model that makes predictions based on the input features.

- **Classification Trees**:
  - A decision tree used for predicting categorical target variables.
  - The leaf nodes contain the most frequent target class of the training examples that fall into that region.

- **Ensemble Trees**:
  - A collection of trees that work together to make predictions.
  - Examples include Random Forest, Gradient Boosting, and AdaBoost.

## Key Concepts

- **Hierarchical Structure**:
  - Trees use a hierarchical structure to represent the decision-making process.
  - Each level of the tree corresponds to a specific feature and its possible values.

- **Recursive Partitioning**:
  - Trees recursively partition the input space into smaller regions based on the input features.
  - This process continues until a stopping criterion is met (e.g., maximum depth, minimum samples per leaf).

- **Feature Importance**:
  - Trees can provide insights into the importance of input features for making predictions.
  - Features that appear higher in the tree are more important for the model's decisions.

- **Interpretability**:
  - Trees are easy to interpret and visualize, making them useful for understanding the model's decision-making process.

- **Handling Non-linear Relationships**:
  - Trees can capture non-linear relationships between input features and the target variable.
  - They can model complex decision boundaries that are not possible with linear models.

## Example

- Consider a dataset of house prices:
  - Features: Square footage, number of bedrooms, neighborhood, age of the house.
  - Target: House price (the value we want to predict).

- A decision tree can learn a set of rules to predict house prices based on the input features.
- For example:
  - If the square footage is less than 1500 and the number of bedrooms is greater than 2, predict a low price.
  - If the square footage is greater than 2000 and the neighborhood is "suburban", predict a high price.

- The tree structure provides insights into the relationships between the input features and the target variable, allowing us to understand how the model makes predictions.

## Advantages and Limitations

- **Advantages**:
  - Interpretable: Easy to understand and explain the decision-making process.
  - Non-linear Relationships: Can capture complex patterns in the data.
  - Feature Importance: Provides insights into the importance of input features.

- **Limitations**:
  - Overfitting: Decision trees can easily overfit noisy data.
  - Instability: Small changes in the data can lead to significantly different tree structures.
  - Lack of Smoothness: Decision boundaries are formed by axis-parallel splits, leading to sharp transitions.
