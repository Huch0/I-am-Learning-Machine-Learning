# Constructing Decision Trees

## Introduction

- **Decision trees** are a popular **supervised learning** algorithm used for **classification** and **regression** tasks.
- They are simple to understand and interpret, making them a popular choice for decision support in various domains.
- Decision trees are often used in combination with other algorithms to form more powerful models, such as **random forests** and **gradient boosting machines**.

## Algorithm

1. **Selecting the Best Split**:
    - The decision tree algorithm starts with the entire dataset as the root node.
    - It then iterates over each feature and evaluates the **information gain** or **Gini impurity** for each possible split.
    - The feature and split that result in the highest information gain or lowest Gini impurity are selected as the best split for the current node.
    - The dataset is then split into two subsets based on the selected feature and split point.
    - The algorithm recursively applies this process to each subset until a stopping criterion is met.

2. **Stopping Criterion**:
    - The decision tree algorithm uses a stopping criterion to determine when to stop splitting the dataset.
    - Common stopping criteria include:
      - Maximum depth of the tree.
      - Minimum number of samples required to split a node.
      - Minimum number of samples required in a leaf node.
      - Maximum number of leaf nodes in the tree.

3. **Tree Pruning**:
    - After the decision tree is constructed, it may be pruned to reduce overfitting and improve generalization.
    - Pruning involves removing nodes from the tree that do not provide significant improvements in prediction accuracy.
    - Pruning can be done using techniques such as **cost complexity pruning** and **reduced error pruning**.

4. **Prediction**:

    - To make predictions, the decision tree algorithm traverses the tree from the root node to a leaf node based on the feature values of the input sample.
    - The predicted class label or value is then determined based on the majority class in the leaf node (for classification) or the average value in the leaf node (for regression).

## Calculating Information

- **Information gain** is a measure of the reduction in entropy or impurity achieved by splitting a dataset based on a particular feature.
- It is used to select the best split at each node of the decision tree.
- The self-information of a random variable $X$ is defined as:

  $$I(X) = -\log_2(P(X))$$

  where:
  - $I(X)$ is the self-information of $X$.
  - $P(X)$ is the probability of $X$ occurring.

- The entropy of a random variable $X$ is defined as the expected value of the self-information:

    $$H(X) = E[I(X)] = -\sum_{i=1}^{n} P(x_i) \log_2(P(x_i))$$

    where:
  - $H(X)$ is the entropy of $X$.
  - $P(x_i)$ is the probability of outcome $x_i$.
  - $n$ is the number of possible outcomes.

- The information gain of a feature $A$ with respect to a target variable $Y$ is defined as the difference between the entropy of $Y$ and the conditional entropy of $Y$ given $A$:

  $$IG(Y, A) = H(Y) - H(Y|A)$$

  where:
  - $IG(Y, A)$ is the information gain of $A$ with respect to $Y$.
  - $H(Y)$ is the entropy of $Y$.
  - $H(Y|A)$ is the conditional entropy of $Y$ given $A$.

- **Gini impurity** is another measure of impurity used in decision trees, especially in the context of classification.
- It is defined as the probability of misclassifying a randomly chosen element if it were randomly labeled according to the distribution of labels in the node.
- The Gini impurity of a node is given by:

  $$Gini(p) = 1 - \sum_{i=1}^{n} p_i^2$$

  where:
  - $Gini(p)$ is the Gini impurity of the node.
  - $p_i$ is the probability of class $i$ in the node.

- The Gini impurity of a split is the weighted sum of the Gini impurities of the child nodes:

    $$Gini_{split} = \sum_{i=1}^{k} \frac{N_i}{N} Gini(p_i)$$

    where:
  - $Gini_{split}$ is the Gini impurity of the split.
  - $N_i$ is the number of samples in the $i$-th child node.
  - $N$ is the total number of samples.

## Example of Calculating Information

- Let's consider an example to understand how information gain is calculated in a decision tree.

- Suppose we have a dataset with the following attributes:

  - Outlook: Sunny, Overcast, Rainy
  - Temperature: Hot, Mild, Cool
  - Humidity: High, Normal
  - Windy: True, False
  - Play: Yes, No

- The goal is to predict whether to play outside based on the weather conditions.

- We want to calculate the information gain of the attribute "Outlook" with respect to the target variable "Play".

- The entropy of the target variable "Play" is given by:

  $$H(Play) = -P(Yes) \log_2(P(Yes)) - P(No) \log_2(P(No))$$

  where:
  - $P(Yes)$ is the probability of playing outside.
  - $P(No)$ is the probability of not playing outside.

- The conditional entropy of "Play" given "Outlook" is given by:

$$H(Play|Outlook) = \sum_{i=1}^{3} P(Outlook = x_i) H(Play|Outlook = x_i)$$

where:

- $P(Outlook = x_i)$ is the probability of each value of "Outlook".
- $H(Play|Outlook = x_i)$ is the entropy of "Play" given each value of "Outlook".

- The information gain of "Outlook" with respect to "Play" is given by:

$$IG(Play, Outlook) = H(Play) - H(Play|Outlook)$$

## Highly Branching Attributes

- Information gain measure prefers attributes with a large numbers of possible values.
- This is because the entropy of the target variable given a highly branching attribute is more likely to be reduced by the split.
- As a result, decision trees may be biased towards features with more levels, which can lead to overfitting.

- **Gain ratio** is a modification of information gain that adjusts for the number of branches of an attribute.
- It is defined as the ratio of information gain to the **split information** of an attribute:

  $$GainRatio(Y, A) = \frac{IG(Y, A)}{SplitInfo(A)}$$

  where:
  - $GainRatio(Y, A)$ is the gain ratio of $A$ with respect to $Y$.
  - $IG(Y, A)$ is the information gain of $A$ with respect to $Y$.
  - $SplitInfo(A)$ is the split information of $A$.

- The split information of an attribute $A$ is defined as the entropy of the distribution of $A$:

$$SplitInfo(A) = -\sum_{i=1}^{n} \frac{N_i}{N} \log_2\left(\frac{N_i}{N}\right)$$

where:

- $SplitInfo(A)$ is the split information of $A$.
- $N_i$ is the number of samples in the $i$-th branch of $A$.
- $N$ is the total number of samples.

- Choose the attribute with maximum gain ratio, provided that its information gain is above a certain threshold.

## Advantages and Disadvantages

- **Advantages**:
  - Simple to understand and interpret.
  - Can handle both numerical and categorical data.
  - Requires little data preprocessing.
  - Performs well with large datasets.
  - Can be used for feature selection and ranking.

- **Disadvantages**:
  - Prone to overfitting, especially when the tree is deep.
  - Can be sensitive to small variations in the data.
  - Biased towards features with more levels.
  - May not be suitable for tasks where the relationships between features are complex.

## Extensions

- **Random Forests**:
  - Random forests are an ensemble learning method that combines multiple decision trees to improve predictive performance and reduce overfitting.

- **Gradient Boosting Machines**:
  - Gradient boosting machines are another ensemble learning method that builds decision trees in a sequential manner to correct the errors of the previous trees.

- **XGBoost**:
  - XGBoost is an optimized implementation of gradient boosting that is widely used in machine learning competitions and real-world applications.

- **LightGBM**:
  - LightGBM is a gradient boosting framework that uses a novel tree learning algorithm to achieve high performance and efficiency.

- **CatBoost**:
  - CatBoost is a gradient boosting library that is designed to handle categorical features and improve predictive performance.
