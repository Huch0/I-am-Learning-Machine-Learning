{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #2 IRIS 데이터를 이용한 Deep Neural Network 모델 설계\n",
    "\n",
    "- 202055623\n",
    "- 허치영"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "## pytorch device 설정\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "iris = fetch_ucirepo(id=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 53, 'name': 'Iris', 'repository_url': 'https://archive.ics.uci.edu/dataset/53/iris', 'data_url': 'https://archive.ics.uci.edu/static/public/53/data.csv', 'abstract': 'A small classic dataset from Fisher, 1936. One of the earliest known datasets used for evaluating classification methods.\\n', 'area': 'Biology', 'tasks': ['Classification'], 'characteristics': ['Tabular'], 'num_instances': 150, 'num_features': 4, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1936, 'last_updated': 'Tue Sep 12 2023', 'dataset_doi': '10.24432/C56C76', 'creators': ['R. A. Fisher'], 'intro_paper': {'title': 'The Iris data set: In search of the source of virginica', 'authors': 'A. Unwin, K. Kleinman', 'published_in': 'Significance, 2021', 'year': 2021, 'url': 'https://www.semanticscholar.org/paper/4599862ea877863669a6a8e63a3c707a787d5d7e', 'doi': '1740-9713.01589'}, 'additional_info': {'summary': 'This is one of the earliest datasets used in the literature on classification methods and widely used in statistics and machine learning.  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is linearly separable from the other 2; the latter are not linearly separable from each other.\\n\\nPredicted attribute: class of iris plant.\\n\\nThis is an exceedingly simple domain.\\n\\nThis data differs from the data presented in Fishers article (identified by Steve Chadwick,  spchadwick@espeedaz.net ).  The 35th sample should be: 4.9,3.1,1.5,0.2,\"Iris-setosa\" where the error is in the fourth feature. The 38th sample: 4.9,3.6,1.4,0.1,\"Iris-setosa\" where the errors are in the second and third features.  ', 'purpose': 'N/A', 'funded_by': None, 'instances_represent': 'Each instance is a plant', 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': None, 'citation': None}}\n",
      "           name     role         type demographic  \\\n",
      "0  sepal length  Feature   Continuous        None   \n",
      "1   sepal width  Feature   Continuous        None   \n",
      "2  petal length  Feature   Continuous        None   \n",
      "3   petal width  Feature   Continuous        None   \n",
      "4         class   Target  Categorical        None   \n",
      "\n",
      "                                         description units missing_values  \n",
      "0                                               None    cm             no  \n",
      "1                                               None    cm             no  \n",
      "2                                               None    cm             no  \n",
      "3                                               None    cm             no  \n",
      "4  class of iris plant: Iris Setosa, Iris Versico...  None             no  \n"
     ]
    }
   ],
   "source": [
    "# data (as pandas dataframes)\n",
    "X = iris.data.features\n",
    "y = iris.data.targets\n",
    "\n",
    "# metadata\n",
    "print(iris.metadata)\n",
    "\n",
    "# variable information\n",
    "print(iris.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         class\n",
       "0  Iris-setosa\n",
       "1  Iris-setosa\n",
       "2  Iris-setosa\n",
       "3  Iris-setosa\n",
       "4  Iris-setosa"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# target이 문자열이므로 LabelEncoder()를 사용하여 숫자로 변환\n",
    "\n",
    "le = LabelEncoder()\n",
    "display(y[:5])\n",
    "y = le.fit_transform(np.ravel(y))\n",
    "display(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train mean:  4.357625371653739e-16\n",
      "X_train std:  1.0\n",
      "X_test mean:  0.0914914534439311\n",
      "X_test std:  0.9984293963082908\n"
     ]
    }
   ],
   "source": [
    "# Train : Test = 8:2로 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# X_train에 대해 fit() 메서드를 호출하여 평균과 표준편차 계산\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# X_train과 X_test data transform\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# scaling 결과 확인\n",
    "print(\"X_train mean: \", X_train.mean())\n",
    "print(\"X_train std: \", X_train.std())\n",
    "print(\"X_test mean: \", X_test.mean())\n",
    "print(\"X_test std: \", X_test.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `LabelEn₩coder`를 이용하여 target을 숫자로 변환\n",
    "- `StandardScaler`를 이용하여 feature를 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # input layer : 4 -> 64\n",
    "        # dataset의 input feature가 4개이므로 input layer의 노드 수는 4\n",
    "        self.input_layer = nn.Linear(4, 64)\n",
    "        # hidden layer : 64 -> 64\n",
    "        self.hidden_layer0 = nn.Linear(64, 64)\n",
    "        self.hidden_layer1 = nn.Linear(64, 64)\n",
    "        # output layer : 64 -> 3\n",
    "        # dataset의 target class가 3개이므로 output layer의 노드 수는 3\n",
    "        self.output_layer = nn.Linear(64, 3)\n",
    "        # activation function : tanh\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.tanh(self.input_layer(x))\n",
    "        x = self.tanh(self.hidden_layer0(x))\n",
    "        x = self.tanh(self.hidden_layer1(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (input_layer): Linear(in_features=4, out_features=64, bias=True)\n",
      "  (hidden_layer0): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (hidden_layer1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (output_layer): Linear(in_features=64, out_features=3, bias=True)\n",
      "  (tanh): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MLP().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch 라이브러리를 이용\n",
    "- `nn.Module`을 상속받아 모델을 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01, Epochs: 100\n",
      "Fold [1/4]\n",
      "Epoch [0/100] Train Loss: 0.30380526185035706\n",
      "Epoch [20/100] Train Loss: 0.0046497792936861515\n",
      "Epoch [40/100] Train Loss: 0.1716327965259552\n",
      "Epoch [60/100] Train Loss: 0.0005343916127458215\n",
      "Epoch [80/100] Train Loss: 0.0005504761938937008\n",
      "Validation Loss: 0.053488049656152725\n",
      "Fold [2/4]\n",
      "Epoch [0/100] Train Loss: 0.2779849171638489\n",
      "Epoch [20/100] Train Loss: 0.01555101852864027\n",
      "Epoch [40/100] Train Loss: 0.004436887335032225\n",
      "Epoch [60/100] Train Loss: 0.024087172001600266\n",
      "Epoch [80/100] Train Loss: 0.6337399482727051\n",
      "Validation Loss: 0.12411072850227356\n",
      "Fold [3/4]\n",
      "Epoch [0/100] Train Loss: 0.9727568626403809\n",
      "Epoch [20/100] Train Loss: 0.005414227023720741\n",
      "Epoch [40/100] Train Loss: 0.005305140744894743\n",
      "Epoch [60/100] Train Loss: 0.4388662874698639\n",
      "Epoch [80/100] Train Loss: 0.0009737040963955224\n",
      "Validation Loss: 0.06812217086553574\n",
      "Fold [4/4]\n",
      "Epoch [0/100] Train Loss: 0.5507019758224487\n",
      "Epoch [20/100] Train Loss: 0.019667144864797592\n",
      "Epoch [40/100] Train Loss: 0.001693958998657763\n",
      "Epoch [60/100] Train Loss: 0.0035855784080922604\n",
      "Epoch [80/100] Train Loss: 0.005521878600120544\n",
      "Validation Loss: 0.14187365770339966\n",
      "Average Validation Loss: 0.14187365770339966\n",
      "--------------------------------------------------\n",
      "Learning Rate: 0.001, Epochs: 100\n",
      "Fold [1/4]\n",
      "Epoch [0/100] Train Loss: 0.9147207140922546\n",
      "Epoch [20/100] Train Loss: 0.267891526222229\n",
      "Epoch [40/100] Train Loss: 0.7584981322288513\n",
      "Epoch [60/100] Train Loss: 0.08036092668771744\n",
      "Epoch [80/100] Train Loss: 0.030954746529459953\n",
      "Validation Loss: 0.09440876543521881\n",
      "Fold [2/4]\n",
      "Epoch [0/100] Train Loss: 0.9202442765235901\n",
      "Epoch [20/100] Train Loss: 0.28871405124664307\n",
      "Epoch [40/100] Train Loss: 0.4310204088687897\n",
      "Epoch [60/100] Train Loss: 0.2459067702293396\n",
      "Epoch [80/100] Train Loss: 0.01638118177652359\n",
      "Validation Loss: 0.18078969419002533\n",
      "Fold [3/4]\n",
      "Epoch [0/100] Train Loss: 1.0373343229293823\n",
      "Epoch [20/100] Train Loss: 0.6129240989685059\n",
      "Epoch [40/100] Train Loss: 0.2742726504802704\n",
      "Epoch [60/100] Train Loss: 0.1072927862405777\n",
      "Epoch [80/100] Train Loss: 0.4587689936161041\n",
      "Validation Loss: 0.12959904968738556\n",
      "Fold [4/4]\n",
      "Epoch [0/100] Train Loss: 1.1233406066894531\n",
      "Epoch [20/100] Train Loss: 0.5312073230743408\n",
      "Epoch [40/100] Train Loss: 0.18294616043567657\n",
      "Epoch [60/100] Train Loss: 0.18448349833488464\n",
      "Epoch [80/100] Train Loss: 0.19684860110282898\n",
      "Validation Loss: 0.13763250410556793\n",
      "Average Validation Loss: 0.13763250410556793\n",
      "--------------------------------------------------\n",
      "Learning Rate: 0.0001, Epochs: 100\n",
      "Fold [1/4]\n",
      "Epoch [0/100] Train Loss: 1.3264613151550293\n",
      "Epoch [20/100] Train Loss: 0.9270045757293701\n",
      "Epoch [40/100] Train Loss: 1.0637232065200806\n",
      "Epoch [60/100] Train Loss: 1.049068808555603\n",
      "Epoch [80/100] Train Loss: 0.360109806060791\n",
      "Validation Loss: 0.5205507874488831\n",
      "Fold [2/4]\n",
      "Epoch [0/100] Train Loss: 1.1013513803482056\n",
      "Epoch [20/100] Train Loss: 1.1158347129821777\n",
      "Epoch [40/100] Train Loss: 0.8090953826904297\n",
      "Epoch [60/100] Train Loss: 0.5983977913856506\n",
      "Epoch [80/100] Train Loss: 0.7573452591896057\n",
      "Validation Loss: 0.6365004181861877\n",
      "Fold [3/4]\n",
      "Epoch [0/100] Train Loss: 1.2345476150512695\n",
      "Epoch [20/100] Train Loss: 0.9985519647598267\n",
      "Epoch [40/100] Train Loss: 0.9229781627655029\n",
      "Epoch [60/100] Train Loss: 0.7206022143363953\n",
      "Epoch [80/100] Train Loss: 0.8066684007644653\n",
      "Validation Loss: 0.6654647588729858\n",
      "Fold [4/4]\n",
      "Epoch [0/100] Train Loss: 1.1328154802322388\n",
      "Epoch [20/100] Train Loss: 0.7519745826721191\n",
      "Epoch [40/100] Train Loss: 1.0463908910751343\n",
      "Epoch [60/100] Train Loss: 0.4437706470489502\n",
      "Epoch [80/100] Train Loss: 0.6161080598831177\n",
      "Validation Loss: 0.5992814302444458\n",
      "Average Validation Loss: 0.5992814302444458\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# K-Fold Cross Validation 수행\n",
    "# k = 4 (Train : Val : Test = 6 : 2 : 2)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "# best model\n",
    "best_model = None\n",
    "best_loss = None\n",
    "best_params = None\n",
    "\n",
    "# hyperparameters\n",
    "params = [\n",
    "    {\"learning_rate\": 0.01, \"epochs\": 100},\n",
    "    {\"learning_rate\": 0.001, \"epochs\": 100},\n",
    "    {\"learning_rate\": 0.0001, \"epochs\": 100},\n",
    "]\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# batch size = 1 (Stochastic Gradient Descent)\n",
    "batch_size = 1\n",
    "\n",
    "for param in params:\n",
    "    learning_rate = param[\"learning_rate\"]\n",
    "    epochs = param[\"epochs\"]\n",
    "\n",
    "    print(f\"Learning Rate: {learning_rate}, Epochs: {epochs}\")\n",
    "    # training loop\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "        print(f\"Fold [{fold+1}/{skf.get_n_splits()}]\")\n",
    "        # train set, validation set 분할\n",
    "        X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        # Pytorch Tensor로 변환\n",
    "        X_train_fold = torch.tensor(X_train_fold, dtype=torch.float32).to(device)\n",
    "        y_train_fold = torch.tensor(y_train_fold, dtype=torch.long).to(device)\n",
    "\n",
    "        # DataLoader 생성\n",
    "        train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # validation loss를 저장할 list\n",
    "        val_losses = []\n",
    "\n",
    "        # model, optimizer 초기화\n",
    "        model = MLP().to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()  # 학습 모드로 설정\n",
    "            # batch_size(1) 만큼씩 데이터를 가져옴\n",
    "            for inputs, targets in train_loader:\n",
    "                optimizer.zero_grad()  # optimizer gradient 초기화\n",
    "                # forward pass\n",
    "                y_pred = model(inputs)\n",
    "                loss = criterion(y_pred, targets)  # loss 계산\n",
    "                # backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()  # weight 업데이트\n",
    "\n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"Epoch [{epoch}/{epochs}] Train Loss: {loss.item()}\")\n",
    "\n",
    "        # validation loss 계산\n",
    "        model.eval()\n",
    "        X_val_tensor = torch.tensor(X_val_fold, dtype=torch.float32).to(device)\n",
    "        y_val_tensor = torch.tensor(y_val_fold, dtype=torch.long).to(device)\n",
    "        y_pred = model(X_val_tensor)\n",
    "        loss = criterion(y_pred, y_val_tensor)\n",
    "        val_losses.append(loss.item())\n",
    "        print(f\"Validation Loss: {loss.item()}\")\n",
    "\n",
    "    # average validation loss 계산\n",
    "    val_losses = np.array(val_losses)\n",
    "    avg_val_loss = val_losses.mean()\n",
    "    print(f\"Average Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "    # best model, best loss , best params 저장\n",
    "    if best_loss is None or avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        best_model = model\n",
    "        best_params = param\n",
    "        \n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4-fold cross validation 진행 (train : val : test = 6 : 2 : 2)\n",
    "- `StratifiedKFold`를 이용하여 class 분포를 유지\n",
    "- Loss function : CrossEntropyLoss\n",
    "- Optimizer : Stoachastic Gradient Descent\n",
    "\n",
    "### best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.001, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set으로 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.06601735949516296\n",
      "Test Accuracy: 1.0\n",
      "          predicted           target\n",
      "0   Iris-versicolor  Iris-versicolor\n",
      "1       Iris-setosa      Iris-setosa\n",
      "2    Iris-virginica   Iris-virginica\n",
      "3   Iris-versicolor  Iris-versicolor\n",
      "4   Iris-versicolor  Iris-versicolor\n",
      "5       Iris-setosa      Iris-setosa\n",
      "6   Iris-versicolor  Iris-versicolor\n",
      "7    Iris-virginica   Iris-virginica\n",
      "8   Iris-versicolor  Iris-versicolor\n",
      "9   Iris-versicolor  Iris-versicolor\n",
      "10   Iris-virginica   Iris-virginica\n",
      "11      Iris-setosa      Iris-setosa\n",
      "12      Iris-setosa      Iris-setosa\n",
      "13      Iris-setosa      Iris-setosa\n",
      "14      Iris-setosa      Iris-setosa\n",
      "15  Iris-versicolor  Iris-versicolor\n",
      "16   Iris-virginica   Iris-virginica\n",
      "17  Iris-versicolor  Iris-versicolor\n",
      "18  Iris-versicolor  Iris-versicolor\n",
      "19   Iris-virginica   Iris-virginica\n",
      "20      Iris-setosa      Iris-setosa\n",
      "21   Iris-virginica   Iris-virginica\n",
      "22      Iris-setosa      Iris-setosa\n",
      "23   Iris-virginica   Iris-virginica\n",
      "24   Iris-virginica   Iris-virginica\n",
      "25   Iris-virginica   Iris-virginica\n",
      "26   Iris-virginica   Iris-virginica\n",
      "27   Iris-virginica   Iris-virginica\n",
      "28      Iris-setosa      Iris-setosa\n",
      "29      Iris-setosa      Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "best_model.eval() # 평가 모드로 설정\n",
    "# Pytorch Tensor로 변환\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "y_pred = best_model(X_test_tensor)\n",
    "loss = criterion(y_pred, y_test_tensor) # loss 계산\n",
    "\n",
    "print(f\"Test Loss: {loss.item()}\")\n",
    "_, predicted = torch.max(y_pred, 1) # 예측값\n",
    "correct = (predicted == y_test_tensor).sum().item() # 정답 개수\n",
    "accuracy = correct / y_test_tensor.size(0) # Categorical Accuracy 계산\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# 예측 결과 출력\n",
    "df = pd.DataFrame()\n",
    "df[\"predicted\"] = le.inverse_transform(predicted.cpu().numpy())\n",
    "df[\"target\"] = le.inverse_transform(y_test)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]             320\n",
      "              Tanh-2                   [-1, 64]               0\n",
      "            Linear-3                   [-1, 64]           4,160\n",
      "              Tanh-4                   [-1, 64]               0\n",
      "            Linear-5                   [-1, 64]           4,160\n",
      "              Tanh-6                   [-1, 64]               0\n",
      "            Linear-7                    [-1, 3]             195\n",
      "================================================================\n",
      "Total params: 8,835\n",
      "Trainable params: 8,835\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.04\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(best_model, (4,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
